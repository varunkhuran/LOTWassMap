{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pl\n",
    "import ot\n",
    "import ot.plot\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some global figure settings\n",
    "%matplotlib inline\n",
    "params = {'legend.fontsize': 'large',\n",
    "          'figure.figsize': (14, 7),\n",
    "         'axes.labelsize': 'X-large',\n",
    "         'axes.titlesize':'X-large',\n",
    "         'xtick.labelsize':'large',\n",
    "         'ytick.labelsize':'large',\n",
    "        'axes.titlesize': 16\n",
    "         }\n",
    "\n",
    "pl.rcParams.update(params)\n",
    "\n",
    "#Make sure the figures directory exists\n",
    "try:\n",
    "    os.makedirs('./figures')\n",
    "except: \n",
    "    print('Figures folder exists')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class and functions for LOT WassMap.\n",
    "\n",
    "A Class that encapsulates the information for an LOT Wassmap experiment with Gaussian data and reference measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_OTmap(xr, xt, a=None, b=None, M=None, \n",
    "               sinkhorn=False, lambd=1, normalize_T=True):\n",
    "    # calculates barycentric projection of OT map\n",
    "    m, dr = xr.shape\n",
    "    n, dt = xt.shape\n",
    "    if a is None:\n",
    "        a = np.ones((m,))/m\n",
    "    if b is None:\n",
    "        b = np.ones((n,))/n\n",
    "        \n",
    "    if M is None:\n",
    "        assert dr == dt\n",
    "        M = ot.dist(xr, xt) # cost matrix for Wasserstein distance computation\n",
    "    \n",
    "    if sinkhorn:\n",
    "        # compute Sinkhorn plan\n",
    "        G = ot.sinkhorn(a, b, M, lambd)\n",
    "    else:\n",
    "        # compute LP plan\n",
    "        G = ot.emd(a, b, M)\n",
    "    # row-normalize plan\n",
    "    Gstochastic = G/G.sum(axis=1)[:,None]\n",
    "    T = Gstochastic @ xt # calculate barycentric projection\n",
    "    if normalize_T:\n",
    "        T = T/np.sqrt(m)\n",
    "    return T\n",
    "    \n",
    "def compute_embedding(transportMat, Ynorm=None):\n",
    "    # Computes embedding with optimal rotation\n",
    "    # compute mean for centering of transport matrix\n",
    "    rowmean = transportMat.mean(axis=0, keepdims=True) \n",
    "    transportMatCentered = transportMat - rowmean # center transport matrix\n",
    "     # SVD of centered transport matrix\n",
    "    u, s, vh = np.linalg.svd(transportMatCentered, full_matrices=False)\n",
    "    u = u[:,:2] @ np.diag(s[:2])\n",
    "    embedding = u\n",
    "    if Ynorm is not None:\n",
    "        unorm = u\n",
    "        innerprod = Ynorm.T @ unorm\n",
    "        u2,s2,vh2 = np.linalg.svd(innerprod)\n",
    "        rotation_matrix = u2 @ vh2\n",
    "        # Rotate embedding\n",
    "        embedding = embedding @ rotation_matrix.T\n",
    "    return embedding\n",
    "\n",
    "def calculate_embedding(xr, xt_list, Y=None, a=None, \n",
    "                        b_list=None, partial=1,lambd=1):\n",
    "    # calculates all embeddings from a reference measure and list of target measures\n",
    "    N = len(xt_list)\n",
    "    m, d = xr.shape\n",
    "    \n",
    "    transportMat = np.zeros((N//partial, m*d)) # initialize matrix of transport maps\n",
    "    entropictransportMat =  np.zeros((N//partial, m*d))\n",
    "    #Y = np.zeros((N//partial, 2)) \n",
    "    for i in range(N//partial):\n",
    "        # get data for OT map calculations\n",
    "        xt = xt_list[i]\n",
    "        b = None\n",
    "        if b_list is not None:\n",
    "            b = b_list[i]\n",
    "        # calculate OT maps\n",
    "        T = calc_OTmap(xr, xt, a=a, b=b, sinkhorn=False)\n",
    "        entropicT = calc_OTmap(xr, xt, a=a, b=b, sinkhorn=True,lambd=1)\n",
    "        # fill in transport matrix\n",
    "        transportMat[i,:] = T.flatten('F')\n",
    "        entropictransportMat[i,:] = entropicT.flatten('F')\n",
    "    # calculate embeddings\n",
    "    embedding = compute_embedding(transportMat, Ynorm=Y)\n",
    "    entropicembedding = compute_embedding(entropictransportMat, Ynorm=Y)\n",
    "    return embedding, entropicembedding\n",
    "\n",
    "def calculate_embedding_LP(xr, xt_list, Y=None, a=None, \n",
    "                        b_list=None, partial=1,lambd=1):\n",
    "    # calculates all embeddings from a reference measure and list of target measures\n",
    "    tic = time.time()\n",
    "    N = len(xt_list)\n",
    "    m, d = xr.shape\n",
    "    \n",
    "    transportMat = np.zeros((N//partial, m*d)) # initialize matrix of transport maps\n",
    "    #Y = np.zeros((N//partial, 2)) \n",
    "    for i in range(N//partial):\n",
    "        # get data for OT map calculations\n",
    "        xt = xt_list[i]\n",
    "        b = None\n",
    "        if b_list is not None:\n",
    "            b = b_list[i]\n",
    "        # calculate OT maps\n",
    "        T = calc_OTmap(xr, xt, a=a, b=b, sinkhorn=False)\n",
    "        # fill in transport matrix\n",
    "        transportMat[i,:] = T.flatten('F')\n",
    "    # calculate embeddings\n",
    "    embedding = compute_embedding(transportMat, Ynorm=Y)\n",
    "    toc = time.time()\n",
    "    timing = toc-tic\n",
    "    return embedding#, timing\n",
    "\n",
    "def calculate_embedding_Sinkhorn(xr, xt_list, Y=None, a=None, \n",
    "                        b_list=None, partial=1,lambd=1):\n",
    "    # calculates all embeddings from a reference measure and list of target measures\n",
    "    tic = time.time()\n",
    "    N = len(xt_list)\n",
    "    m, d = xr.shape\n",
    "    \n",
    "    entropictransportMat = np.zeros((N//partial, m*d)) # initialize matrix of transport maps\n",
    "    #Y = np.zeros((N//partial, 2)) \n",
    "    for i in range(N//partial):\n",
    "        # get data for OT map calculations\n",
    "        xt = xt_list[i]\n",
    "        b = None\n",
    "        if b_list is not None:\n",
    "            b = b_list[i]\n",
    "        # calculate OT maps\n",
    "        T = calc_OTmap(xr, xt, a=a, b=b, sinkhorn=True, lambd = lambd)\n",
    "        # fill in transport matrix\n",
    "        entropictransportMat[i,:] = T.flatten('F')\n",
    "    # calculate embeddings\n",
    "    entropicembedding = compute_embedding(entropictransportMat, Ynorm=Y)\n",
    "    toc = time.time()\n",
    "    timing = toc-tic\n",
    "    return entropicembedding#, timing\n",
    "\n",
    "\n",
    "def wass_matrix(xt_list,a = None, b_list = None, squared=True,p=2.0):\n",
    "    \"\"\"\n",
    "    The function compute the (squared if squared=True) Wasserstein Distance Matrix between N images\n",
    "    image_list: python list of pointcloud representations \n",
    "    \"\"\"\n",
    "    N = len(xt_list) #number of images\n",
    "    wass_dist = np.zeros((N,N)) #initialize the distance matrix\n",
    "    for i in range(N):\n",
    "        for j in range(i+1,N):\n",
    "            n, dt = xt_list[i].shape\n",
    "            if a is None:\n",
    "                a = np.ones((n,))/n\n",
    "            if b_list[i] is not None:\n",
    "                b = b_list[i]\n",
    "            M = np.power(ot.dist(xt_list[i], xt_list[j],'euclidean'),p)\n",
    "            wass_dist[i,j] = ot.emd2(a,b,M)\n",
    "            \n",
    "    wass_dist += wass_dist.T\n",
    "\n",
    "    if(squared==True):\n",
    "        wass_dist = np.square(wass_dist) \n",
    "    return wass_dist\n",
    "\n",
    "def calculate_MDS_embedding(xt_list, a = None, b_list = None, squared=True,num_components = 2):\n",
    "    \"\"\"\n",
    "    The function computes the MDS embedding from a set of data measures.\n",
    "    \"\"\"\n",
    "    N = len(xt_list)\n",
    "    H = np.eye(N)-1/N*np.ones((N,N))\n",
    "    dist_matrix = wass_matrix(xt_list, a = a, b_list = b_list)\n",
    "    if squared==False:\n",
    "        B = -.5*H@(dist_matrix**2)@H\n",
    "    else:\n",
    "        B = -.5*H@dist_matrix@H\n",
    "    U,S,VT = np.linalg.svd(B)\n",
    "    embedding = U[:,:num_components]@np.diag(S[:num_components]**.5)\n",
    "\n",
    "    return embedding\n",
    "    \n",
    "    \n",
    "\n",
    "class LOT_WassMap_Gauss(object):\n",
    "    \n",
    "    def __init__(self, m, n, N, mu_r, cov_r, mu_t_list, cov_t_list, Y=None, partial=1,\n",
    "                b_list = None, lambd=1, c = pl.cm.tab10, figs = True):\n",
    "        self.m = m\n",
    "        self.n = n\n",
    "        self.N = N\n",
    "        self.a = np.ones((self.m,)) / self.m\n",
    "        self.partial = 1\n",
    "        self.Y = np.zeros((self.N//self.partial, 2)) \n",
    "        self.b_list = []\n",
    "        self.lambd = lambd\n",
    "        self.c = c(np.linspace(0,1,N))\n",
    "        self.figs = figs\n",
    "        # empirical sample of size m of reference measure\n",
    "        self.xr = ot.datasets.make_2D_samples_gauss(m, mu_r, cov_r)\n",
    "        self.xt_list = []\n",
    "        # create Gauss\n",
    "        for i in range(N//self.partial):\n",
    "            mu_t = mu_t_list[i]\n",
    "            self.Y[i,:] = mu_t\n",
    "            self.b_list.append( np.ones((self.n,))/self.n )\n",
    "            cov_t = cov_t_list[i]\n",
    "            xt = ot.datasets.make_2D_samples_gauss(self.n, mu_t, cov_t)\n",
    "            self.xt_list.append(xt)\n",
    "        if Y is not None:\n",
    "            self.Y = Y\n",
    "        if b_list is not None:\n",
    "            self.b_list = b_list\n",
    "            \n",
    "    def calc_LOT_WassMap(self,):\n",
    "        \n",
    "        # create embeddings\n",
    "        self.embedding, self.entropicembedding = calculate_embedding(self.xr,\n",
    "                                            self.xt_list, Y=self.Y,\n",
    "                                            a=self.a, b_list=self.b_list, lambd=self.lambd,\n",
    "                                            partial=self.partial)\n",
    "        \n",
    "        err = np.linalg.norm(self.Y-self.embedding,'fro')/np.linalg.norm(self.Y,'fro')\n",
    "        entropic_error = np.linalg.norm(self.Y-self.entropicembedding)/np.linalg.norm(self.Y)\n",
    "        \n",
    "        # If figs is true, the data and reference distributions are plotted along with the embeddings\n",
    "        if self.figs:\n",
    "            fig = pl.figure(figsize=(14,7))\n",
    "            ax = fig.add_subplot(141)\n",
    "            # plot reference distribution\n",
    "            ax.scatter(self.xr[:, 0], self.xr[:, 1], marker='+',\n",
    "                       color='blue', label='Source samples')\n",
    "            ax.set_title('Data')\n",
    "            ax.set_aspect('equal')\n",
    "            for i in range(N//partial):\n",
    "                xt = self.xt_list[i]\n",
    "                # add plot of data measure to existing plot\n",
    "                ax.scatter(xt[:, 0], xt[:, 1], \n",
    "                           marker='x',c=[self.c[i]]*self.n,\n",
    "                        label='Target samples')\n",
    "\n",
    "            # plot reference distribution\n",
    "            ax2 = fig.add_subplot(142)\n",
    "            ax2.scatter(Y[:, 0], Y[:, 1], c=self.c)\n",
    "            ax2.set_title('True Embedding Points')\n",
    "            ax2.set_aspect('equal')\n",
    "\n",
    "\n",
    "            # Plot final rotated embedding\n",
    "            ax3 = fig.add_subplot(143)\n",
    "            ax3.scatter(self.embedding[:, 0], self.embedding[:, 1], c=self.c)\n",
    "            ax3.set_title('Linear Program Embedding')\n",
    "            ax3.set_aspect('equal')\n",
    "\n",
    "            # Plot final rotated embedding\n",
    "            ax4 = fig.add_subplot(144)\n",
    "            ax4.scatter(self.entropicembedding[:, 0], self.entropicembedding[:, 1], c=self.c)\n",
    "            ax4.set_title('Sinkhorn Embedding')\n",
    "            ax4.set_aspect('equal')\n",
    "            fig.tight_layout(pad=5.0)\n",
    "            pl.show()\n",
    "        \n",
    "        return self.embedding, self.entropicembedding, err, entropic_error\n",
    "    \n",
    "    def LOT_timing(self,):\n",
    "        # For timing MDS vs. LOT with Linear Program vs. LOT with Sinkhorn\n",
    "        tic = time.time()\n",
    "        self.embedding = calculate_embedding_LP(self.xr,self.xt_list,Y=self.Y,\n",
    "                                            a=self.a, b_list=self.b_list, lambd=self.lambd,\n",
    "                                            partial=self.partial)\n",
    "        toc = time.time()\n",
    "        LP_timing = toc-tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        self.entropicembedding = calculate_embedding_Sinkhorn(self.xr,self.xt_list,Y=self.Y,\n",
    "                                            a=self.a, b_list=self.b_list, lambd=self.lambd,\n",
    "                                            partial=self.partial)\n",
    "        toc = time.time()\n",
    "        Sinkhorn_timing = toc-tic\n",
    "        \n",
    "        tic = time.time()\n",
    "        self.MDSembedding = calculate_MDS_embedding(self.xt_list,a=self.a,b_list=self.b_list)\n",
    "        toc = time.time()\n",
    "        MDS_timing = toc-tic\n",
    "        \n",
    "        return self.embedding, LP_timing, self.entropicembedding, Sinkhorn_timing, self.MDSembedding, MDS_timing\n",
    "    \n",
    "    def LOT_timing_LP(self,):\n",
    "        # For timing LOT with the Linear Program used to compute transport maps\n",
    "        \n",
    "        tic = time.time()\n",
    "        self.embedding = calculate_embedding_LP(self.xr,self.xt_list,Y=self.Y,\n",
    "                                            a=self.a, b_list=self.b_list, lambd=self.lambd,\n",
    "                                            partial=self.partial)\n",
    "        toc = time.time()\n",
    "        LP_timing = toc-tic\n",
    "        \n",
    "        return self.embedding, LP_timing\n",
    "    \n",
    "    def LOT_timing_Sinkhorn(self, lambd = None):\n",
    "        # For timing LOT with the Sinkhorn used to compute transport maps\n",
    "        \n",
    "        if lambd is None:\n",
    "            lambd = self.lambd\n",
    "        \n",
    "        tic = time.time()\n",
    "        self.entropicembedding = calculate_embedding_Sinkhorn(self.xr,self.xt_list,Y=self.Y,\n",
    "                                            a=self.a, b_list=self.b_list, lambd=lambd,\n",
    "                                            partial=self.partial)\n",
    "        toc = time.time()\n",
    "        Sinkhorn_timing = toc-tic\n",
    "        \n",
    "        return self.entropicembedding, Sinkhorn_timing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-D Translations with Perturbations\n",
    "Experiment 1 and Figures 1 and 2 from the paper. We use a standard normal reference distribution, and the data is a set of $N=10 $ Gaussians with means on a circle of radius 4, all having the same dependent covariance matrix which is contaminated with random Gaussia noise. Each measure is sampled uniformly.\n",
    "\n",
    "Note that when the reference and data measures are sampled sparsely, the embedding will contain some scaling distortion. For larger sample sizes (>100 or so) the scale of the embedding is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "m = 1000      # Number of samples of reference measure\n",
    "n = m        # Number of samples of data measures\n",
    "lambd = 1 # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "radius = 8 # set radius governing means of data points (larger is more visible, but performance is similar for all)\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([radius*np.cos(2*np.pi*(i-1)/N), radius*np.sin(2*np.pi*(i-1)/N)]) # Mean centered on circle of radius radius\n",
    "    cov_t = np.array([[1, -.5], [-.5, 1]]) # correlated covariance matrix\n",
    "    errMat = noise_level * np.random.rand(d,d)  # random Guassian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    # empirical sample of size n of data measure\n",
    "    Y[i,:] = mu_t\n",
    "\n",
    "    \n",
    "lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial,figs=True,lambd=lambd)\n",
    "embedding, entropic_embedding, err, entropic_error = lot_WassMap.calc_LOT_WassMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing experiment for the circle translation example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "mlist = np.arange(50,850,50) # list with number of samples m of the reference measure\n",
    "nlist = mlist # list with number of samples n of data measures -- default the same as m\n",
    "numiter = 10  # Number of iterations (each trial of the LOT Wassmap algorithm is run numiter times and error is averaged)\n",
    "lambd = 1 # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "radius = 8 # set radius governing means of data points (larger is more visible, but performance is similar for all)\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([radius*np.cos(2*np.pi*(i-1)/N), radius*np.sin(2*np.pi*(i-1)/N)]) # Mean centered on circle of radius radius\n",
    "    cov_t = np.array([[1, -.5], [-.5, 1]]) # correlated covariance matrix\n",
    "    errMat = noise_level * np.random.rand(d,d)  # random Guassian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    # empirical sample of size n of data measure\n",
    "    Y[i,:] = mu_t\n",
    "\n",
    "\n",
    "errStore_1dtrans = np.zeros((len(mlist),len(nlist), numiter))\n",
    "errStoreEntropic_1dtrans = np.zeros((len(mlist),len(nlist), numiter))\n",
    "\n",
    "for repiter in range(numiter):\n",
    "    for miter in range(len(mlist)):\n",
    "        for niter in range(len(nlist)): \n",
    "\n",
    "            m = mlist[miter]\n",
    "            n = nlist[niter]\n",
    "            \n",
    "            lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial,figs=False)\n",
    "            embedding, entropic_embedding, err, entropic_error = lot_WassMap.calc_LOT_WassMap()\n",
    "            \n",
    "            errStore_1dtrans[miter,niter,repiter] = err\n",
    "            errStoreEntropic_1dtrans[miter,niter,repiter] = entropic_error\n",
    "\n",
    "errMean_1dtrans = np.mean(errStore_1dtrans[0,:,:],axis=1)\n",
    "errStd_1dtrans = np.std(errStore_1dtrans[0,:,:],axis=1)\n",
    "\n",
    "Entropic_errorMean_1dtrans = np.mean(errStoreEntropic_1dtrans[0,:,:],axis=1)\n",
    "Entropic_errorStd_1dtrans = np.std(errStoreEntropic_1dtrans[0,:,:],axis=1)\n",
    "\n",
    "fig = pl.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(nlist,errMean_1dtrans,'b',marker='o')\n",
    "ax.fill_between(nlist,errMean_1dtrans - errStd_1dtrans,errMean_1dtrans + errStd_1dtrans,color = 'gray',alpha=.2)\n",
    "ax.set_title('Linear Program')\n",
    "ax.set_xticks(np.arange(100,900,100))\n",
    "ax.set_xlabel('Sample size $m$')\n",
    "ax.set_ylabel('Relative Error')\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(nlist,Entropic_errorMean_1dtrans,'b',marker='o')\n",
    "ax2.fill_between(nlist,Entropic_errorMean_1dtrans - Entropic_errorStd_1dtrans,Entropic_errorMean_1dtrans + Entropic_errorStd_1dtrans,color = 'gray',alpha=.2)\n",
    "ax2.set_title('Sinkhorn')\n",
    "ax2.set_xticks(np.arange(100,900,100))\n",
    "ax2.set_xlabel('Sample size $m$')\n",
    "ax2.set_ylabel('Relative Error')\n",
    "#ax2.set_aspect('equal')\n",
    "fig.tight_layout(pad=5.0)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotations with Perturbations\n",
    "Experiment 2 and Figures 3 and 4 from the paper. We use a standard normal reference distribution, and the data is a set of N=10 Gaussians with means on a circle of radius 8, where the dependent covariance matrices are also rotated according to the rotation angle; in this way, we form a 1-dimensional rotation manifold governed by the circle. Each measure is sampled uniformly.\n",
    "\n",
    "Note that when the reference and data measures are sampled sparsely, the embedding will contain some scaling distortion. For larger sample sizes (>100 or so) the scale of the embedding is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 10      # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "m = 1000      # Number of samples of reference measure\n",
    "n = m        # Number of samples of data measures\n",
    "lambd = 1    # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "radius = 8 # set radius governing means of data points (larger is more visible, but performance is similar for all)\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "## Create data measures and add to plot of reference distribution\n",
    "for i in range(N//partial):\n",
    "    angle = 2*np.pi*(i-1)/N   # angles uniformly sampling the unit circle\n",
    "    angle_vec = np.array([np.cos(angle), np.sin(angle)])   # mean corresponds to points on the circle of radius r\n",
    "    mu_t = angle_vec*radius\n",
    "    cov_t = np.array([[2, 0], [0, 0.5]])   # covariance matrix\n",
    "    rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],[np.sin(angle),np.cos(angle)]]) # form rotation matrix\n",
    "    cov_t = rotation_matrix @ cov_t @ rotation_matrix.T  # add rotation to covariance matrix\n",
    "    \n",
    "    errMat = noise_level * np.random.rand(d,d) # random Gaussian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    \n",
    "    Y[i,:] = mu_t\n",
    "    \n",
    "lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial)\n",
    "lot_WassMap.calc_LOT_WassMap()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing experiment for the rotation manifold example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 10      # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "mlist = np.arange(50,850,50)\n",
    "nlist = mlist\n",
    "numiter = 10  # Number of iterations (each trial of the LOT Wassmap algorithm is run numiter times and error is averaged)\n",
    "lambd = 1    # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "radius = 8 # set radius governing means of data points (larger is more visible, but performance is similar for all)\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "## Create data measures and add to plot of reference distribution\n",
    "for i in range(N//partial):\n",
    "    angle = 2*np.pi*(i-1)/N   # angles uniformly sampling the unit circle\n",
    "    angle_vec = np.array([np.cos(angle), np.sin(angle)])   # mean corresponds to points on the circle of radius r\n",
    "    mu_t = angle_vec*radius\n",
    "    cov_t = np.array([[2, 0], [0, 0.5]])   # covariance matrix\n",
    "    rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],[np.sin(angle),np.cos(angle)]]) # form rotation matrix\n",
    "    cov_t = rotation_matrix @ cov_t @ rotation_matrix.T  # add rotation to covariance matrix\n",
    "    \n",
    "    errMat = noise_level * np.random.rand(d,d) # random Gaussian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    \n",
    "    Y[i,:] = mu_t\n",
    "\n",
    "errStore = np.zeros((len(mlist),len(nlist), numiter))\n",
    "errStoreEntropic = np.zeros((len(mlist),len(nlist), numiter))\n",
    "\n",
    "for repiter in range(numiter):\n",
    "    for miter in range(len(mlist)):\n",
    "        for niter in range(len(nlist)): \n",
    "\n",
    "            m = mlist[miter]\n",
    "            n = nlist[niter]\n",
    "            \n",
    "            lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial,figs=False)\n",
    "            embedding, entropic_embedding, err, entropic_error = lot_WassMap.calc_LOT_WassMap()\n",
    "            \n",
    "            errStore[miter,niter,repiter] = err\n",
    "            errStoreEntropic[miter,niter,repiter] =entropic_error\n",
    "\n",
    "errMean = np.mean(errStore[0,:,:],axis=1)\n",
    "errStd = np.std(errStore[0,:,:],axis=1)\n",
    "\n",
    "Entropic_errorMean = np.mean(errStoreEntropic[0,:,:],axis=1)\n",
    "Entropic_errorStd = np.std(errStoreEntropic[0,:,:],axis=1)\n",
    "\n",
    "fig = pl.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(nlist,errMean,'b',marker='o')\n",
    "ax.fill_between(nlist,errMean - errStd,errMean + errStd,color = 'gray',alpha=.2)\n",
    "ax.set_title('Linear Program')\n",
    "ax.set_xticks(np.arange(100,900,100))\n",
    "ax.set_xlabel('Sample size $m$')\n",
    "ax.set_ylabel('Relative Error')\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(nlist,Entropic_errorMean,'b',marker='o')\n",
    "ax2.fill_between(nlist,Entropic_errorMean - Entropic_errorStd,Entropic_errorMean + Entropic_errorStd,color = 'gray',alpha=.2)\n",
    "ax2.set_title('Sinkhorn')\n",
    "ax2.set_xticks(np.arange(100,900,100))\n",
    "ax2.set_xlabel('Sample size $m$')\n",
    "ax2.set_ylabel('Relative Error')\n",
    "#ax2.set_aspect('equal')\n",
    "fig.tight_layout(pad=5.0)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-D Translations with Perturbations\n",
    "Experiment 3 and Figures 5 and 6 from the paper. We use a standard normal reference distribution, and the data is a set of N=10 Gaussians with means on a uniform 5x5 grid on the cube $[-10,10]^2$, all having the same dependent covariance matrix which are contaminated with random Gaussian noise. Each measure is sampled uniformly.\n",
    "\n",
    "Note that when the reference and data measures are sampled sparsely, the embedding will contain some scaling distortion. For larger sample sizes (>100 or so) the scale of the embedding is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 25       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "m = 1000      # Number of samples of reference measure\n",
    "n = m        # Number of samples of data measures\n",
    "lambd = 1e1    # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "# form coordinate grid for the 2-d translation lattice\n",
    "x = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "y = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "# full coordinate arrays\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "xx = xx.reshape(-1)\n",
    "yy = yy.reshape(-1)\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([xx[i], yy[i]]) # mean on the sampled 2-D lattice\n",
    "    cov_t = np.array([[1, -.5], [-.5, 1]]) # correlated covariance matrix\n",
    "    errMat = noise_level * np.random.rand(d,d) # random Gaussian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    \n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    \n",
    "    Y[i,:] = mu_t\n",
    "    \n",
    "\n",
    "lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial, lambd=lambd, c = pl.cm.turbo)\n",
    "embedding, entropic_embedding, err, entropic_error = lot_WassMap.calc_LOT_WassMap()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing experiment for the grid translation example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 25       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "mlist = np.arange(50,850,50)\n",
    "nlist = mlist\n",
    "numiter = 10  # Number of iterations (each trial of the LOT Wassmap algorithm is run numiter times and error is averaged)\n",
    "\n",
    "lambd = 1e1    # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "# form coordinate grid for the 2-d translation lattice\n",
    "x = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "y = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "# full coordinate arrays\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "xx = xx.reshape(-1)\n",
    "yy = yy.reshape(-1)\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([xx[i], yy[i]]) # mean on the sampled 2-D lattice\n",
    "    cov_t = np.array([[1, -.5], [-.5, 1]]) # correlated covariance matrix\n",
    "    errMat = noise_level * np.random.rand(d,d) # random Gaussian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    \n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    \n",
    "    Y[i,:] = mu_t\n",
    "    \n",
    "errStore_2dtrans = np.zeros((len(mlist),len(nlist), numiter))\n",
    "errStoreEntropic_2dtrans = np.zeros((len(mlist),len(nlist), numiter))\n",
    "\n",
    "for repiter in range(numiter):\n",
    "    for miter in range(len(mlist)):\n",
    "        for niter in range(len(nlist)): \n",
    "\n",
    "            m = mlist[miter]\n",
    "            n = nlist[niter]\n",
    "            \n",
    "            lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial,figs=False)\n",
    "            embedding, entropic_embedding, err, entropic_error = lot_WassMap.calc_LOT_WassMap()\n",
    "            \n",
    "            errStore_2dtrans[miter,niter,repiter] = err\n",
    "            errStoreEntropic_2dtrans[miter,niter,repiter] =entropic_error\n",
    "\n",
    "errMean_2dtrans = np.mean(errStore_2dtrans[0,:,:],axis=1)\n",
    "errStd_2dtrans = np.std(errStore_2dtrans[0,:,:],axis=1)\n",
    "\n",
    "Entropic_errorMean_2dtrans = np.mean(errStoreEntropic_2dtrans[0,:,:],axis=1)\n",
    "Entropic_errorStd_2dtrans = np.std(errStoreEntropic_2dtrans[0,:,:],axis=1)\n",
    "\n",
    "fig = pl.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(nlist,errMean_2dtrans,'b',marker='o')\n",
    "ax.fill_between(nlist,errMean_2dtrans - errStd_2dtrans,errMean_2dtrans + errStd_2dtrans,color = 'gray',alpha=.2)\n",
    "ax.set_title('Linear Program')\n",
    "ax.set_xticks(np.arange(100,900,100))\n",
    "ax.set_xlabel('Sample size $m$')\n",
    "ax.set_ylabel('Relative Error')\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(nlist,Entropic_errorMean_2dtrans,'b',marker='o')\n",
    "ax2.fill_between(nlist,Entropic_errorMean_2dtrans - Entropic_errorStd_2dtrans,Entropic_errorMean_2dtrans + Entropic_errorStd_2dtrans,color = 'gray',alpha=.2)\n",
    "ax2.set_title('Sinkhorn')\n",
    "ax2.set_xticks(np.arange(100,900,100))\n",
    "ax2.set_xlabel('Sample size $m$')\n",
    "ax2.set_ylabel('Relative Error')\n",
    "#ax2.set_aspect('equal')\n",
    "fig.tight_layout(pad=5.0)\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anisotropic Dilation\n",
    "\n",
    "Experiment 4 and Figures 7 and 8 from the paper. We use a standard normal reference distribution, and the data is a set of $N=10 $ Gaussians with mean 0 and scaled covariance matrices which are contaminated with random Gaussian noise. Each measure is sampled uniformly.\n",
    "\n",
    "Note that when the reference and data measures are sampled sparsely, the embedding will contain some scaling distortion. For larger sample sizes (>100 or so) the scale of the embedding is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 9       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "m = 1000      # Number of samples of reference measure\n",
    "n = 2500       # Number of samples of data measures\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]]) # reference covariance matrix\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "noise_level = 0.5 #.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "scaling = np.linspace(1, 4, num=int(np.sqrt(N)))\n",
    "scaling = np.flip(scaling)\n",
    "XX, YY = np.meshgrid(scaling,scaling)\n",
    "XX = XX.reshape(-1)\n",
    "YY = YY.reshape(-1)\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([0, 0])\n",
    "    #cov_t = np.array([[1, -.5], [-.5, 1]])\n",
    "    cov_t = np.array([[1, 0], [0, 1]])\n",
    "    errMat = noise_level * np.random.rand(d,d)\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat)\n",
    "    cov_t = np.diag([XX[i],YY[i]])@cov_t@np.diag([XX[i],YY[i]])\n",
    "    \n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "\n",
    "    Y[i,0] = XX[i] - np.mean(XX)\n",
    "    Y[i,1] = YY[i] - np.mean(YY)\n",
    "\n",
    "lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial, lambd = 1e2)\n",
    "embedding, entropic_embedding, err, entropic_error = lot_WassMap.calc_LOT_WassMap()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing experiment for the dilation manifold above. Note that dilation appears to be more difficult, and if small values of $m$ are used, there may be more distortion in the embeddings than for more simple articulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 9       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "# m = 1000      # Number of samples of reference measure\n",
    "# n = 2500       # Number of samples of data measures\n",
    "mlist = np.arange(500,2100,250)\n",
    "nlist = 2500\n",
    "numiter = 10  # Number of iterations (each trial of the LOT Wassmap algorithm is run numiter times and error is averaged)\n",
    "\n",
    "lambd = 1e1    # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "#cov_r = .1*np.array([[1, -.5], [-.5, 1]])  # reference covariance matrix\n",
    "cov_r = np.array([[1, 0], [0, 1]])\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "noise_level = 0.5 #.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "scaling = np.linspace(1, 4, num=int(np.sqrt(N)))\n",
    "scaling = np.flip(scaling)\n",
    "XX, YY = np.meshgrid(scaling,scaling)\n",
    "XX = XX.reshape(-1)\n",
    "YY = YY.reshape(-1)\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([0, 0])\n",
    "    #cov_t = np.array([[1, -.5], [-.5, 1]])\n",
    "    cov_t = np.array([[1, 0], [0, 1]])\n",
    "    errMat = noise_level * np.random.rand(d,d)\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat)\n",
    "    cov_t = np.diag([XX[i],YY[i]])@cov_t@np.diag([XX[i],YY[i]])\n",
    "    \n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "\n",
    "    Y[i,0] = XX[i] - np.mean(XX)\n",
    "    Y[i,1] = YY[i] - np.mean(YY)\n",
    "\n",
    "errStore_dilation = np.zeros((len(mlist), numiter))\n",
    "errStoreEntropic_dilation = np.zeros((len(mlist), numiter))\n",
    "\n",
    "\n",
    "for repiter in range(numiter):\n",
    "    for miter in range(len(mlist)):\n",
    "\n",
    "        m = mlist[miter]\n",
    "        n = m\n",
    "\n",
    "        lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                        cov_t_list, Y=Y, partial=partial, lambd = 1e2,figs=False)\n",
    "        embedding, entropic_embedding, err, entropic_error = lot_WassMap.calc_LOT_WassMap()    \n",
    "\n",
    "        errStore_dilation[miter,repiter] = err\n",
    "        errStoreEntropic_dilation[miter,repiter] = entropic_error\n",
    "\n",
    "errMean_dilation = np.mean(errStore_dilation,axis=1)\n",
    "errStd_dilation = np.std(errStore_dilation,axis=1)\n",
    "\n",
    "Entropic_errorMean_dilation = np.mean(errStoreEntropic_dilation,axis=1)\n",
    "Entropic_errorStd_dilation = np.std(errStoreEntropic_dilation,axis=1)\n",
    "\n",
    "fig = pl.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(121)\n",
    "ax.plot(mlist,errMean_dilation,'b',marker='o')\n",
    "ax.fill_between(mlist,errMean_dilation - errStd_dilation,errMean_dilation + errStd_dilation,color = 'gray',alpha=.2)\n",
    "ax.set_title('Linear Program')\n",
    "ax.set_xticks(mlist)\n",
    "ax.set_xlabel('Sample size $m$')\n",
    "ax.set_ylabel('Relative Error')\n",
    "# ax.set_aspect('equal')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "ax2.plot(mlist,Entropic_errorMean_dilation,'b',marker='o')\n",
    "ax2.fill_between(mlist,Entropic_errorMean_dilation - Entropic_errorStd_dilation,Entropic_errorMean_dilation + Entropic_errorStd_dilation,color = 'gray',alpha=.2)\n",
    "ax2.set_title('Sinkhorn')\n",
    "ax2.set_xticks(mlist)\n",
    "ax2.set_xlabel('Sample size $m$')\n",
    "ax2.set_ylabel('Relative Error')\n",
    "#ax2.set_aspect('equal')\n",
    "fig.tight_layout(pad=5.0)\n",
    "pl.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing Experiment\n",
    "Experiment 5 and Figures 9 and 10 in the paper. Here we use the grid translation example to time traditional Wassmap against LOT Wassmap using both the linear program and Sinkhorn solvers to compute the optimal tranpsort maps. The first cell times LP, Sinkhorn with $\\lambda=1$ and Wassmap. The second cell times LP and Sinkhorn with $\\lambda=1$ and $\\lambda=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 25       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "# m = 1000      # Number of samples of reference measure\n",
    "# n = m        # Number of samples of data measures\n",
    "mlist = np.arange(100,850,100)\n",
    "nlist = mlist\n",
    "numiter =10  # Number of iterations (each trial of the LOT Wassmap algorithm is run numiter times and error is averaged)\n",
    "\n",
    "lambd = 1e1    # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "# form coordinate grid for the 2-d translation lattice\n",
    "x = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "y = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "# full coordinate arrays\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "xx = xx.reshape(-1)\n",
    "yy = yy.reshape(-1)\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([xx[i], yy[i]]) # mean on the sampled 2-D lattice\n",
    "    cov_t = np.array([[1, -.5], [-.5, 1]]) # correlated covariance matrix\n",
    "    errMat = noise_level * np.random.rand(d,d) # random Gaussian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    \n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    \n",
    "    Y[i,:] = mu_t\n",
    "    \n",
    "timeStore_2dtrans = np.zeros((len(mlist), numiter))\n",
    "timeStoreEntropic_2dtrans = np.zeros((len(mlist), numiter))\n",
    "timeStoreMDS_2dtrans = np.zeros((len(mlist), numiter))\n",
    "\n",
    "for repiter in range(numiter):\n",
    "    for miter in range(len(mlist)): \n",
    "\n",
    "            m = mlist[miter]\n",
    "            n = m\n",
    "            \n",
    "            lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial,figs=False,lambd=lambd)\n",
    "            embedding, LP_time, entropic_embedding, entropic_time, MDS_embedding, MDS_time = lot_WassMap.LOT_timing()\n",
    "            \n",
    "            timeStore_2dtrans[miter,repiter] = LP_time\n",
    "            timeStoreEntropic_2dtrans[miter,repiter] = entropic_time\n",
    "            timeStoreMDS_2dtrans[miter,repiter] = MDS_time\n",
    "\n",
    "timeMean_2dtrans = np.mean(timeStore_2dtrans, axis = 1)\n",
    "timeStd_2dtrans = np.std(timeStore_2dtrans, axis = 1)\n",
    "\n",
    "Entropic_timeMean_2dtrans = np.mean(timeStoreEntropic_2dtrans, axis = 1)\n",
    "Entropic_timeStd_2dtrans = np.std(timeStoreEntropic_2dtrans, axis = 1)\n",
    "\n",
    "MDS_timeMean_2dtrans = np.mean(timeStoreMDS_2dtrans, axis = 1)\n",
    "MDS_timeStd_2dtrans = np.std(timeStoreMDS_2dtrans, axis = 1)\n",
    "\n",
    "\n",
    "fig = pl.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(111)\n",
    "line1, = ax.plot(mlist,timeMean_2dtrans,color = 'cyan',marker='o')\n",
    "ax.fill_between(mlist,timeMean_2dtrans - timeStd_2dtrans,timeMean_2dtrans + timeStd_2dtrans,color = 'cyan',alpha=.1)\n",
    "line2, = ax.plot(mlist,Entropic_timeMean_2dtrans,color = 'magenta',marker='o')\n",
    "ax.fill_between(mlist,Entropic_timeMean_2dtrans - Entropic_timeStd_2dtrans,Entropic_timeMean_2dtrans + Entropic_timeStd_2dtrans,color = 'magenta',alpha=.1)\n",
    "line3, = ax.plot(mlist,MDS_timeMean_2dtrans,color='red',marker='o')\n",
    "ax.fill_between(mlist,MDS_timeMean_2dtrans - MDS_timeStd_2dtrans,MDS_timeMean_2dtrans + MDS_timeStd_2dtrans,color = 'red',alpha=.1)\n",
    "\n",
    "ax.set_title('Timing')\n",
    "ax.set_xticks(mlist)\n",
    "ax.set_xlabel('Sample size $m$')\n",
    "ax.set_ylabel('(Time) (s)')\n",
    "ax.legend([line1,line2,line3],[\"LP\",\"Sinkhorn ($\\lambda=1$)\",\"Wassmap\"])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat of previous experiment for timing only LOT Wassmap with LP and Sinkhorn for various $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Parameters ####\n",
    "N = 25       # Number of data points\n",
    "partial = 1  # Subsampling parameter of data points (if desired)\n",
    "d = 2        # ambient data dimension\n",
    "# m = 1000      # Number of samples of reference measure\n",
    "# n = m        # Number of samples of data measures\n",
    "mlist = np.arange(1000,2000,100)\n",
    "nlist = mlist\n",
    "numiter =10  # Number of iterations (each trial of the LOT Wassmap algorithm is run numiter times and error is averaged)\n",
    "\n",
    "#lambd = 1e1    # entropic regularization parameter\n",
    "\n",
    "#### Generate Reference Measure ####\n",
    "## Reference measure is an empirical sample of N(0,I)\n",
    "\n",
    "a, b = np.ones((m,)) / m, np.ones((n,)) / n  # uniform distribution on samples\n",
    "\n",
    "mu_r = np.array([0, 0])   # mean 0\n",
    "cov_r = np.array([[1, 0], [0, 1]])  # Identity covariance matrix\n",
    "\n",
    "\n",
    "Y = np.zeros((N//partial, 2)) \n",
    "\n",
    "noise_level = 0.5 # noise level (standard deviation) added to covariance matrix of data measures\n",
    "\n",
    "mu_t_list = []\n",
    "cov_t_list = []\n",
    "\n",
    "# form coordinate grid for the 2-d translation lattice\n",
    "x = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "y = np.linspace(-10, 10, int(np.sqrt(N)))\n",
    "# full coordinate arrays\n",
    "xx, yy = np.meshgrid(x, y)\n",
    "xx = xx.reshape(-1)\n",
    "yy = yy.reshape(-1)\n",
    "\n",
    "for i in range(N//partial):\n",
    "    mu_t = np.array([xx[i], yy[i]]) # mean on the sampled 2-D lattice\n",
    "    cov_t = np.array([[1, -.5], [-.5, 1]]) # correlated covariance matrix\n",
    "    errMat = noise_level * np.random.rand(d,d) # random Gaussian noise\n",
    "    cov_t = cov_t + errMat @ np.transpose(errMat) # Add symmetric noise to covariance matrix\n",
    "    \n",
    "    mu_t_list.append(mu_t)\n",
    "    cov_t_list.append(cov_t)\n",
    "    \n",
    "    Y[i,:] = mu_t\n",
    "    \n",
    "\n",
    "timeStore_rotation_LP = np.zeros((len(mlist), numiter))\n",
    "# timeStoreEntropic_rotation_lambda01 = np.zeros((len(mlist), numiter))\n",
    "timeStoreEntropic_rotation_lambda1 = np.zeros((len(mlist), numiter))\n",
    "timeStoreEntropic_rotation_lambda10 = np.zeros((len(mlist), numiter))\n",
    "# timeStoreEntropic_rotation_lambda100 = np.zeros((len(mlist), numiter))\n",
    "# timeStoreEntropic_rotation_lambda1000 = np.zeros((len(mlist), numiter))\n",
    "\n",
    "for repiter in range(numiter):\n",
    "    for miter in range(len(mlist)): \n",
    "\n",
    "            m = mlist[miter]\n",
    "            n = m\n",
    "            \n",
    "            lot_WassMap = LOT_WassMap_Gauss(m, n, N, mu_r, cov_r, mu_t_list,\n",
    "                cov_t_list, Y=Y, partial=partial,figs=False)\n",
    "            embedding, LP_time = lot_WassMap.LOT_timing_LP()\n",
    "            timeStore_rotation_LP[miter,repiter] = LP_time\n",
    "            \n",
    "#             entropicembedding, Sinkhorn_time_lambda01 = lot_WassMap.LOT_timing_Sinkhorn(lambd = .1)\n",
    "#             timeStoreEntropic_rotation_lambda01[miter, repiter] = Sinkhorn_time_lambda01\n",
    "            \n",
    "            entropicembedding, Sinkhorn_time_lambda1 = lot_WassMap.LOT_timing_Sinkhorn(lambd = 1)\n",
    "            timeStoreEntropic_rotation_lambda1[miter, repiter] = Sinkhorn_time_lambda1\n",
    "            \n",
    "            entropicembedding, Sinkhorn_time_lambda10 = lot_WassMap.LOT_timing_Sinkhorn(lambd = 10)\n",
    "            timeStoreEntropic_rotation_lambda10[miter, repiter] = Sinkhorn_time_lambda10\n",
    "            \n",
    "#             entropicembedding, Sinkhorn_time_lambda100 = lot_WassMap.LOT_timing_Sinkhorn(lambd = 100)\n",
    "#             timeStoreEntropic_rotation_lambda100[miter, repiter] = Sinkhorn_time_lambda100\n",
    "            \n",
    "#             entropicembedding, Sinkhorn_time_lambda1000 = lot_WassMap.LOT_timing_Sinkhorn(lambd = 1000)\n",
    "#             timeStoreEntropic_rotation_lambda1000[miter, repiter] = Sinkhorn_time_lambda1000\n",
    "            \n",
    "\n",
    "timeMean_rotation_LP = np.mean(timeStore_rotation_LP, axis = 1)\n",
    "timeStd_rotation_LP = np.std(timeStore_rotation_LP, axis = 1)\n",
    "\n",
    "# Entropic_timeMean_rotation_lambda01 = np.mean(timeStoreEntropic_rotation_lambda01, axis = 1)\n",
    "# Entropic_timeStd_rotation_lambda01 = np.std(timeStoreEntropic_rotation_lambda01, axis = 1)\n",
    "\n",
    "Entropic_timeMean_rotation_lambda1 = np.mean(timeStoreEntropic_rotation_lambda1, axis = 1)\n",
    "Entropic_timeStd_rotation_lambda1 = np.std(timeStoreEntropic_rotation_lambda1, axis = 1)\n",
    "\n",
    "Entropic_timeMean_rotation_lambda10 = np.mean(timeStoreEntropic_rotation_lambda10, axis = 1)\n",
    "Entropic_timeStd_rotation_lambda10 = np.std(timeStoreEntropic_rotation_lambda10, axis = 1)\n",
    "\n",
    "# Entropic_timeMean_rotation_lambda100 = np.mean(timeStoreEntropic_rotation_lambda100, axis = 1)\n",
    "# Entropic_timeStd_rotation_lambda100 = np.std(timeStoreEntropic_rotation_lambda100, axis = 1)\n",
    "\n",
    "# Entropic_timeMean_rotation_lambda1000 = np.mean(timeStoreEntropic_rotation_lambda1000, axis = 1)\n",
    "# Entropic_timeStd_rotation_lambda1000 = np.std(timeStoreEntropic_rotation_lambda1000, axis = 1)\n",
    "\n",
    "\n",
    "c = pl.cm.tab10(np.linspace(0,1,65))\n",
    "fig = pl.figure(figsize=(12,5))\n",
    "ax = fig.add_subplot(111)\n",
    "line11, = ax.plot(mlist,timeMean_rotation_LP,color = 'cyan', marker='o')\n",
    "ax.fill_between(mlist,timeMean_rotation_LP - timeStd_rotation_LP,timeMean_rotation_LP + timeStd_rotation_LP,color = 'cyan',alpha=.1)\n",
    "\n",
    "# line12, = ax.plot(mlist,Entropic_timeMean_rotation_lambda01,color='red',marker='o')\n",
    "# ax.fill_between(mlist,Entropic_timeMean_rotation_lambda01 - Entropic_timeStd_rotation_lambda01,Entropic_timeMean_rotation_lambda01 + Entropic_timeStd_rotation_lambda01,color = 'red',alpha=.1)\n",
    "\n",
    "line13, = ax.plot(mlist,Entropic_timeMean_rotation_lambda1,color='red',marker='o')\n",
    "ax.fill_between(mlist,Entropic_timeMean_rotation_lambda1 - Entropic_timeStd_rotation_lambda1,Entropic_timeMean_rotation_lambda1 + Entropic_timeStd_rotation_lambda1,color = 'red',alpha=.1)\n",
    "\n",
    "line14, = ax.plot(mlist,Entropic_timeMean_rotation_lambda10,color = 'magenta',marker='o')\n",
    "ax.fill_between(mlist,Entropic_timeMean_rotation_lambda10 - Entropic_timeStd_rotation_lambda10,Entropic_timeMean_rotation_lambda10 + Entropic_timeStd_rotation_lambda10,color = 'magenta',alpha=.1)\n",
    "\n",
    "# line15, = ax.plot(mlist,Entropic_timeMean_rotation_lambda100,color='green',marker='o')\n",
    "# ax.fill_between(mlist,Entropic_timeMean_rotation_lambda100 - Entropic_timeStd_rotation_lambda100,Entropic_timeMean_rotation_lambda100 + Entropic_timeStd_rotation_lambda100,color = 'green',alpha=.1)\n",
    "\n",
    "# line16, = ax.plot(mlist,Entropic_timeMean_rotation_lambda1000,color = 'magenta',marker='o')\n",
    "# ax.fill_between(mlist,Entropic_timeMean_rotation_lambda1000 - Entropic_timeStd_rotation_lambda1000,Entropic_timeMean_rotation_lambda1000 + Entropic_timeStd_rotation_lambda1000,color = 'magenta',alpha=.1)\n",
    "\n",
    "ax.set_title('Timing')\n",
    "ax.set_xticks(mlist)\n",
    "ax.set_xlabel('Sample size $m$')\n",
    "ax.set_ylabel('Time (s)')\n",
    "ax.legend([line11,line13,line14],[\"LP\", \"$\\lambda = 1$\", \"$\\lambda = 10$\"])\n",
    "\n",
    "pl.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
